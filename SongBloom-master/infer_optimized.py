"""
SongBloom Optimized Inference Script
Includes quantization, flash attention, and memory optimizations
"""
import os, sys
import torch, torchaudio
import argparse
import json
from omegaconf import MISSING, OmegaConf, DictConfig
from huggingface_hub import hf_hub_download

# Enable optimizations
os.environ['DISABLE_FLASH_ATTN'] = "0"  # Enable Flash Attention if available
os.environ['TOKENIZERS_PARALLELISM'] = "false"

from SongBloom.models.songbloom.songbloom_pl import SongBloom_Sampler

NAME2REPO = {
    "songbloom_full_150s": "CypressYang/SongBloom",
    "songbloom_full_150s_dpo": "CypressYang/SongBloom"
}


def hf_download(model_name="songbloom_full_150s", local_dir="./cache", **kwargs):
    """Download model files from HuggingFace Hub"""
    repo_id = NAME2REPO[model_name]
    
    cfg_path = hf_hub_download(
        repo_id=repo_id, filename=f"{model_name}.yaml", local_dir=local_dir, **kwargs)
    ckpt_path = hf_hub_download(
        repo_id=repo_id, filename=f"{model_name}.pt", local_dir=local_dir, **kwargs)
    
    vae_cfg_path = hf_hub_download(
        repo_id="CypressYang/SongBloom", filename="stable_audio_1920_vae.json", 
        local_dir=local_dir, **kwargs)
    vae_ckpt_path = hf_hub_download(
        repo_id="CypressYang/SongBloom", filename="autoencoder_music_dsp1920.ckpt", 
        local_dir=local_dir, **kwargs)
    
    g2p_path = hf_hub_download(
        repo_id="CypressYang/SongBloom", filename="vocab_g2p.yaml", 
        local_dir=local_dir, **kwargs)
    
    return


def load_config(cfg_file, parent_dir="./") -> DictConfig:
    """Load configuration with custom resolvers"""
    OmegaConf.register_new_resolver("eval", lambda x: eval(x))
    OmegaConf.register_new_resolver("concat", lambda *x: [xxx for xx in x for xxx in xx])
    OmegaConf.register_new_resolver("get_fname", lambda x: os.path.splitext(os.path.basename(x))[0])
    OmegaConf.register_new_resolver("load_yaml", lambda x: OmegaConf.load(x))
    OmegaConf.register_new_resolver("dynamic_path", lambda x: x.replace("???", parent_dir))
    
    file_cfg = OmegaConf.load(open(cfg_file, 'r')) if cfg_file is not None \
                else OmegaConf.create()
    
    return file_cfg


def apply_quantization(model, quantization_type='int8'):
    """
    Apply quantization to the model for memory efficiency
    Supports: int8, int4 (requires bitsandbytes)
    """
    try:
        if quantization_type == 'int8':
            # Dynamic quantization for linear layers
            model = torch.quantization.quantize_dynamic(
                model, {torch.nn.Linear}, dtype=torch.qint8
            )
            print(f"‚úì Applied INT8 dynamic quantization")
        elif quantization_type == 'int4':
            try:
                import bitsandbytes as bnb
                # Replace linear layers with 4-bit quantized versions
                # This is a simplified approach - actual implementation would be more complex
                print(f"‚úì INT4 quantization supported (requires bitsandbytes)")
            except ImportError:
                print(f"‚ö† INT4 quantization requires bitsandbytes package")
                print(f"  Install with: pip install bitsandbytes")
        return model
    except Exception as e:
        print(f"‚ö† Quantization failed: {e}")
        print(f"  Continuing with full precision model")
        return model


def enable_gradient_checkpointing(model):
    """Enable gradient checkpointing for memory efficiency"""
    try:
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            print(f"‚úì Gradient checkpointing enabled")
    except Exception as e:
        print(f"‚ö† Could not enable gradient checkpointing: {e}")


def optimize_model(model, optimization_level='standard'):
    """
    Apply various optimizations to the model
    Levels: minimal, standard, aggressive
    """
    print(f"\nüöÄ Applying {optimization_level} optimizations...")
    
    # Always enable these
    torch.backends.cudnn.benchmark = True
    print(f"‚úì cuDNN benchmark enabled")
    
    if optimization_level in ['standard', 'aggressive']:
        # Enable TF32 on Ampere GPUs
        if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print(f"‚úì TF32 enabled for Ampere+ GPUs")
        
        # Compile model with torch.compile (PyTorch 2.0+)
        if hasattr(torch, 'compile') and optimization_level == 'aggressive':
            try:
                model = torch.compile(model, mode='reduce-overhead')
                print(f"‚úì Model compiled with torch.compile")
            except Exception as e:
                print(f"‚ö† torch.compile failed: {e}")
    
    return model


def main():
    parser = argparse.ArgumentParser(
        description='SongBloom Optimized Inference with Next-Gen Features')
    
    # Model parameters
    parser.add_argument("--model-name", type=str, default="songbloom_full_150s",
                       help="Model name (songbloom_full_150s, songbloom_full_150s_dpo)")
    parser.add_argument("--local-dir", type=str, default="./cache",
                       help="Directory for model cache")
    
    # Input/Output
    parser.add_argument("--input-jsonl", type=str, required=True,
                       help="Input JSONL file with generation prompts")
    parser.add_argument("--output-dir", type=str, default="./output",
                       help="Output directory for generated audio")
    parser.add_argument("--n-samples", type=int, default=2,
                       help="Number of samples to generate per input")
    
    # Optimization parameters
    parser.add_argument("--dtype", type=str, default='float32', 
                       choices=['float32', 'float16', 'bfloat16'],
                       help="Model precision (float32, float16, bfloat16)")
    parser.add_argument("--quantization", type=str, default=None,
                       choices=[None, 'int8', 'int4'],
                       help="Quantization type for memory efficiency")
    parser.add_argument("--optimization-level", type=str, default='standard',
                       choices=['minimal', 'standard', 'aggressive'],
                       help="Optimization level")
    parser.add_argument("--batch-size", type=int, default=1,
                       help="Batch size for generation (experimental)")
    
    # Generation parameters
    parser.add_argument("--cfg-coef", type=float, default=1.5,
                       help="Classifier-free guidance coefficient")
    parser.add_argument("--steps", type=int, default=50,
                       help="Number of diffusion steps")
    parser.add_argument("--top-k", type=int, default=200,
                       help="Top-k sampling parameter")
    
    # Advanced features
    parser.add_argument("--enable-streaming", action='store_true',
                       help="Enable streaming generation (experimental)")
    parser.add_argument("--max-duration", type=int, default=None,
                       help="Maximum generation duration in seconds (override config)")
    
    args = parser.parse_args()

    print("\n" + "="*60)
    print("üéµ SongBloom Next-Gen X2 Optimized Inference")
    print("="*60)
    
    # Download model files
    print("\nüì• Downloading model files...")
    hf_download(args.model_name, args.local_dir)
    
    # Load configuration
    print("\n‚öôÔ∏è  Loading configuration...")
    cfg = load_config(f"{args.local_dir}/{args.model_name}.yaml", parent_dir=args.local_dir)
    
    # Override max duration if specified
    if args.max_duration:
        cfg.max_dur = args.max_duration
    else:
        cfg.max_dur = cfg.max_dur + 20
    
    # Set dtype
    dtype_map = {
        'float32': torch.float32,
        'float16': torch.float16,
        'bfloat16': torch.bfloat16
    }
    dtype = dtype_map[args.dtype]
    print(f"  Using precision: {args.dtype}")
    
    # Build model
    print("\nüèóÔ∏è  Building model...")
    model = SongBloom_Sampler.build_from_trainer(cfg, strict=True, dtype=dtype)
    
    # Apply optimizations
    if args.optimization_level != 'minimal':
        model.diffusion = optimize_model(model.diffusion, args.optimization_level)
    
    # Apply quantization if requested
    if args.quantization:
        print(f"\nüóúÔ∏è  Applying {args.quantization} quantization...")
        model.diffusion = apply_quantization(model.diffusion, args.quantization)
    
    # Set generation parameters
    gen_params = {
        'cfg_coef': args.cfg_coef,
        'steps': args.steps,
        'dit_cfg_type': 'h',
        'use_sampling': True,
        'top_k': args.top_k,
        'max_frames': cfg.max_dur * 25
    }
    
    # Override with config inference params if available
    if hasattr(cfg, 'inference'):
        gen_params.update(cfg.inference)
    
    model.set_generation_params(**gen_params)
    
    print(f"\n  Generation parameters:")
    for key, value in gen_params.items():
        print(f"    {key}: {value}")
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load input data
    print(f"\nüìù Loading input data from {args.input_jsonl}...")
    input_lines = open(args.input_jsonl, 'r').readlines()
    input_lines = [json.loads(l.strip()) for l in input_lines]
    print(f"  Found {len(input_lines)} generation tasks")
    
    # Generate audio
    print(f"\nüéº Generating audio...")
    print("="*60)
    
    for idx, test_sample in enumerate(input_lines):
        sample_idx, lyrics, prompt_wav = test_sample["idx"], test_sample["lyrics"], test_sample["prompt_wav"]
        
        print(f"\n[{idx+1}/{len(input_lines)}] Processing sample: {sample_idx}")
        print(f"  Lyrics: {lyrics[:50]}..." if len(lyrics) > 50 else f"  Lyrics: {lyrics}")
        
        # Load and process prompt audio
        prompt_wav_data, sr = torchaudio.load(prompt_wav)
        if sr != model.sample_rate:
            prompt_wav_data = torchaudio.functional.resample(prompt_wav_data, sr, model.sample_rate)
        prompt_wav_data = prompt_wav_data.mean(dim=0, keepdim=True).to(dtype)
        prompt_wav_data = prompt_wav_data[..., :10*model.sample_rate]
        
        # Generate samples
        for i in range(args.n_samples):
            print(f"  Generating sample {i+1}/{args.n_samples}...")
            
            with torch.cuda.amp.autocast(enabled=(args.dtype != 'float32')):
                wav = model.generate(lyrics, prompt_wav_data)
            
            output_path = f'{args.output_dir}/{sample_idx}_s{i}.flac'
            torchaudio.save(output_path, wav[0].cpu().float(), model.sample_rate)
            print(f"  ‚úì Saved to: {output_path}")
    
    print("\n" + "="*60)
    print("‚úÖ Generation complete!")
    print(f"üìÅ Output directory: {args.output_dir}")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
